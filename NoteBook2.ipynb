{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd90835-3527-4319-abb8-760756cc72f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f208e0e-8c4c-4441-8e40-25da015a2ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, when, from_utc_timestamp, date_format, split, concat, lit, regexp_replace\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735620fe-78f6-46d5-82b0-184e07db8c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82481cb-789f-4ebd-b9d8-ac6d1f998e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = dbutils.fs.ls(\"/mnt/bronze/ecommerce/\")\n",
    "tableNames = []\n",
    "for table in path:\n",
    "    tableNames.append(table.name.split(\"/\")[0])\n",
    "print(tableNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f96205-27b4-443c-914e-ee568c866c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for i in tableNames:\n",
    "    path = f\"/mnt/bronze/ecommerce/{i}/{i}.parquet\"\n",
    "    df = spark.read.parquet(path)\n",
    "    columns = df.columns\n",
    "    columns_name=[]\n",
    "    # Concatenate first_name and last_name if both exist\n",
    "    if \"first_name\" in columns and \"last_name\" in columns:\n",
    "        df = df.withColumn(\"full_name\", concat(df[\"first_name\"], lit(\" \"), df[\"last_name\"]))\n",
    "\n",
    "    # Rename columns if necessary\n",
    "    if any(\"user_name\" in col for col in columns):\n",
    "        df = df.withColumnRenamed(\"user_name\", \"user_id\")\n",
    "    if any(\"seller_address\" in col for col in columns):\n",
    "        df = df.withColumnRenamed(\"seller_address\", \"address\")\n",
    "    \n",
    "    # Split address into city, state, and zipcode if address exists\n",
    "    if any(\"address\" in col for col in columns):\n",
    "        df = df.withColumn(\"city\", split(df['address'], \",\").getItem(0))\\\n",
    "               .withColumn(\"state\", split(df['address'], \",\").getItem(1))\\\n",
    "               .withColumn(\"zipcode\", split(df['address'], \",\").getItem(2))\n",
    "\n",
    "    if \"product_category\" in columns:\n",
    "        df = df.withColumn(\"product_category_converted\", regexp_replace(\"product_category\", \"_\", \" \"))\n",
    "    if \"payment_type\" in columns:\n",
    "        df= df.withColumn(\"payment_type_converted\", regexp_replace(\"payment_type\", \"_\", \" \"))\n",
    "\n",
    "    df = df.drop(\"address\", \"first_name\", \"last_name\", \"product_photos_qty\", \"product_name_lenght\", \"product_description_lenght\", \"product_category\", \"payment_type\")\n",
    "    \n",
    "    # Store the DataFrame in the dictionary\n",
    "    dfs[i] = df\n",
    "\n",
    "# for i in dfs:\n",
    "#     dfs[i].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15721a82-e0aa-4e22-981e-94d07b44bf74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "converted_dfs = {}\n",
    "for i in dfs:\n",
    "    # path=\"/mnt/silver/ecommerce/\"+i+\"/\"\n",
    "    # df=spark.read.format(\"delta\").load(path,header=True)\n",
    "    df = dfs[i]\n",
    "    columns_name=[]\n",
    "    for col_name in df.columns:\n",
    "        if \"date\" in col_name:\n",
    "            columns_name.append(col_name)\n",
    "        df_converted = df\n",
    "        for col_name in columns_name:\n",
    "            df_converted = df_converted.withColumn(\n",
    "                col_name + \"_converted\",\n",
    "                when(\n",
    "                    to_timestamp(col(col_name), \"MM-dd-yyyy HH:mm\").isNotNull(),\n",
    "                    date_format(to_timestamp(col(col_name), \"MM-dd-yyyy HH:mm\"), \"yyyy-MM-dd HH:mm\")\n",
    "                ).when(\n",
    "                    to_timestamp(col(col_name), \"MM/dd/yyyy HH:mm\").isNotNull(),\n",
    "                    date_format(to_timestamp(col(col_name), \"MM/dd/yyyy HH:mm\"), \"yyyy-MM-dd HH:mm\")\n",
    "                ).when(\n",
    "                    to_timestamp(col(col_name), \"dd-MM-yyyy HH:mm\").isNotNull(),\n",
    "                    date_format(to_timestamp(col(col_name), \"dd-MM-yyyy HH:mm\"), \"yyyy-MM-dd HH:mm\")\n",
    "                ).when(\n",
    "                    to_timestamp(col(col_name), \"M/d/yyyy HH:mm\").isNotNull(),\n",
    "                    date_format(to_timestamp(col(col_name), \"M/d/yyyy HH:mm\"), \"yyyy-MM-dd HH:mm\")\n",
    "                ).when(\n",
    "                    to_timestamp(col(col_name), \"M/d/yyyy H:mm\").isNotNull(),\n",
    "                    date_format(to_timestamp(col(col_name), \"M/d/yyyy H:mm\"), \"yyyy-MM-dd HH:mm\")\n",
    "                ).otherwise(None)\n",
    "            )\n",
    "            \n",
    "        df_converted = df_converted.drop(*columns_name)\n",
    "        converted_dfs[i] = df_converted\n",
    "\n",
    "# for i in converted_dfs:\n",
    "#     converted_dfs[i].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f0eafd-a40d-4d76-8586-f770c8d5f71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for filter_table in converted_dfs:\n",
    "    silver_path = f\"/mnt/silver/ecommerce/{filter_table}/\"\n",
    "    print(filter_table)\n",
    "    converted_dfs[filter_table].write.mode(\"overwrite\").format('delta').save(silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NoteBook2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
